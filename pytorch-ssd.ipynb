{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from PIL import Image, ImageDraw\nimport re\n\nwith open('/kaggle/input/colorful-fashion-dataset-for-object-detection/colorful_fashion_dataset_for_object_detection/Annotations/100932.xml') as f:\n    reader = f.read()\n\nimg = Image.open('/kaggle/input/colorful-fashion-dataset-for-object-detection/colorful_fashion_dataset_for_object_detection/JPEGImages/100932.jpg')\n\nx_min = int(re.findall('(?<=<xmin>)[0-9]+?(?=</xmin>)', reader)[0])\nx_max = int(re.findall('(?<=<xmax>)[0-9]+?(?=</xmax>)', reader)[0])\ny_min = int(re.findall('(?<=<ymin>)[0-9]+?(?=</ymin>)', reader)[0])\ny_max = int(re.findall('(?<=<ymax>)[0-9]+?(?=</ymax>)', reader)[0])\n\nimage = img.copy()\ndraw = ImageDraw.Draw(image)\ndraw.rectangle(xy=[(x_min,y_min), (x_max,y_max)])\nimage","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:16.760615Z","iopub.execute_input":"2022-03-25T06:05:16.761467Z","iopub.status.idle":"2022-03-25T06:05:16.919948Z","shell.execute_reply.started":"2022-03-25T06:05:16.761331Z","shell.execute_reply":"2022-03-25T06:05:16.919289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndimsions=(300, 300)\ndimensions_0 = torch.FloatTensor([img.width, img.height, img.width, img.height]).unsqueeze(0)\nb = torch.FloatTensor([x_min, y_min, x_max, y_max])\nnb = b / dimensions_0 # b = box\ndimsion_1 = torch.FloatTensor([dimsions[1], dimsions[0], dimsions[1], dimsions[0]]).unsqueeze(0)\nnb = nb * dimsion_1","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:17.275205Z","iopub.execute_input":"2022-03-25T06:05:17.275419Z","iopub.status.idle":"2022-03-25T06:05:18.558905Z","shell.execute_reply.started":"2022-03-25T06:05:17.275394Z","shell.execute_reply":"2022-03-25T06:05:18.558094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nimg = transforms.Resize(dimsions)(img)\ndraw = ImageDraw.Draw(img)\ndraw.rectangle(xy=[tuple(nb.tolist()[0])[:2], tuple(nb.tolist()[0])[2:]])\nimg","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:18.560794Z","iopub.execute_input":"2022-03-25T06:05:18.561053Z","iopub.status.idle":"2022-03-25T06:05:18.78735Z","shell.execute_reply.started":"2022-03-25T06:05:18.561017Z","shell.execute_reply":"2022-03-25T06:05:18.786675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass Dateset(Dataset):\n    def __init__(self, folder, test=False, transform=None):\n        self.img_folder = '/kaggle/input/colorful-fashion-dataset-for-object-detection/colorful_fashion_dataset_for_object_detection/JPEGImages/'\n        self.annotation_folder = '/kaggle/input/colorful-fashion-dataset-for-object-detection/colorful_fashion_dataset_for_object_detection/Annotations/'\n        self.folder = folder\n        self.transform = transform\n        self.test = test\n        \n    def __getitem__(self, idx):\n        file = (self.folder[idx] + '.jpg')\n        img_path = self.img_folder + file\n        img = Image.open(img_path)\n        img = img.convert('RGB')\n        \n        if not self.test:\n            annotation_path = (self.annotation_folder + file.split('.')[0] + '.xml')\n            with open(annotation_path) as f:\n                annotation = f.read()\n\n            xy = self.get_xy(annotation)\n            b = torch.FloatTensor(list(xy))\n\n            nb = self.b_resize(b, img)\n            if self.transform is not None:\n                img = self.transform(img)\n\n            return img, nb, torch.FloatTensor([1])\n        else:\n            return img\n    \n    def __len__(self):\n        return len(self.folder)\n        \n    def get_xy(self, annotation):\n        x_min = int(re.findall('(?<=<xmin>)[0-9]+?(?=</xmin>)', annotation)[0])\n        x_max = int(re.findall('(?<=<xmax>)[0-9]+?(?=</xmax>)', annotation)[0])\n        y_min = int(re.findall('(?<=<ymin>)[0-9]+?(?=</ymin>)', annotation)[0])\n        y_max = int(re.findall('(?<=<ymax>)[0-9]+?(?=</ymax>)', annotation)[0])\n\n        return x_min, y_min, x_max, y_max\n    \n    def show_b(self):\n        file = random.choice(self.folder)\n        annotation_path = self.annotation_folder + file.split('.')[0]\n        \n        img_b = Image.open(self.img_folder + file)\n        with open(annotation_path) as f:\n            annotation = f.read()\n            \n        draw = ImageDraw.Draw(img_b)\n        xy = self.get_xy(annotation)\n        print('bbox:', xy)\n        draw.rectangle(xy=[xy[:2], xy[2:]])\n        \n        return img_b\n        \n    def b_resize(self, b, img, dimsion=(300, 300)):\n        dimsion_0 = torch.FloatTensor([img.width, img.height, img.width, img.height]).unsqueeze(0)\n        nb = b / dimsion_0\n        dimsion_1 = torch.FloatTensor([dimsion[1], dimsion[0], dimsion[1], dimsion[0]]).unsqueeze(0)\n\n        return nb\n    \n    def collate_fn(self, batch):\n        \"\"\"\n        to combine these tensors of different sizes. using lists.\n        \"\"\"\n\n        images = list()\n        bxs = list()\n        lbl = list()\n\n        for b in batch:\n            images.append(b[0])\n            bxs.append(b[1])\n            lbl.append(b[2])\n            \n        images = torch.stack(images, dim=0)\n\n        return images, bxs, lbl  # tensor (N, 3, 300, 300), 3 lists of N tensors each\n","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:18.788647Z","iopub.execute_input":"2022-03-25T06:05:18.789087Z","iopub.status.idle":"2022-03-25T06:05:18.808033Z","shell.execute_reply.started":"2022-03-25T06:05:18.78905Z","shell.execute_reply":"2022-03-25T06:05:18.807243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize([300, 300]),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:18.809853Z","iopub.execute_input":"2022-03-25T06:05:18.810263Z","iopub.status.idle":"2022-03-25T06:05:18.822808Z","shell.execute_reply.started":"2022-03-25T06:05:18.810221Z","shell.execute_reply":"2022-03-25T06:05:18.82213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xy_to_ctrx_ctry(xy):\n    \"\"\"\n    Converting bounding boxes from co-ordinates (x_min, y_min, x_max, y_max) to center-sized co-ordinates (ctrx, ctry, w, h).\n    \"\"\"\n    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  \n                      xy[:, 2:] - xy[:, :2]], 1)  \n\n\ndef ctrx_ctry_to_xy(ctrx_ctry):\n    \"\"\"\n    Converting bounding boxes from center-size coordinates (ctrx, ctry, w, h) to boundary coordinates (x_min, y_min, x_max, y_max).\n    \n    \"\"\"\n    return torch.cat([ctrx_ctry[:, :2] - (ctrx_ctry[:, 2:] / 2),  # x_min, y_min\n                      ctrx_ctry[:, :2] + (ctrx_ctry[:, 2:] / 2)], 1)  # x_max, y_max\n\n\ndef ctrx_ctry_to_gcxgcy(ctrx_ctry, prior_cxcy):\n    \"\"\"\n    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n    In the model, we are predicting bounding box coordinates in this encoded form.\n    \"\"\"\n\n    # The 10 and 5 below are referred to as 'variances' in the original Caffe repo, completely empirical\n    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n    return torch.cat([(ctrx_ctry[:, :2] - prior_cxcy[:, :2]) / (prior_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n                      torch.log(ctrx_ctry[:, 2:] / prior_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n\n\ndef gcxgcy_to_ctrx_ctry(gcxgcy, prior_cxcy):\n    \"\"\"\n    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n    They are decoded into center-size coordinates.\n    This is the inverse of the function above.\n    \"\"\"\n\n    return torch.cat([gcxgcy[:, :2] * prior_cxcy[:, 2:] / 10 + prior_cxcy[:, :2],  # c_x, c_y\n                      torch.exp(gcxgcy[:, 2:] / 5) * prior_cxcy[:, 2:]], 1)  # w, h\n\ndef find_IoU(set_1, set_2):\n    \"\"\"\n    Find Intersection over Union of every box combination between two sets of boxes that are in boundary coordinates.\n    \"\"\"\n\n    # Find intersections\n    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)\n\ndef find_intersection(set_1, set_2):\n    \"\"\"\n    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n    \"\"\"\n\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\ndef decimate(tensor, m):\n    \"\"\"\n    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n    \"\"\"\n    assert tensor.dim() == len(m)\n    for d in range(tensor.dim()):\n        if m[d] is not None:\n            tensor = tensor.index_select(dim=d, index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n\n    return tensor\n\ndef calculate_mAP(det_box, det_lbl, det_score, true_box, true_lbl, true_difficulties):\n    \"\"\"\n    Calculate the Mean Average Precision (mAP) of detected objects.\n    https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 \n    \"\"\"\n    assert len(det_box) == len(det_lbl) == len(det_score) == len(true_box) == len(true_lbl) == len(true_difficulties)  \n    # these are all lists of tensors of the same length, i.e. number of images\n    n_classes = len(label_map)\n\n    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n    true_images = list()\n    for i in range(len(true_lbl)):\n        true_images.extend([i] * true_lbl[i].size(0))\n    true_images = torch.LongTensor(true_images).to(device)  # (n_objects), n_objects is the total no. of objects across all images\n    true_box = torch.cat(true_box, dimsion=0)  # (n_objects, 4)\n    true_lbl = torch.cat(true_lbl, dimsion=0)  # (n_objects)\n    true_difficulties = torch.cat(true_difficulties, dimsion=0)  # (n_objects)\n\n    assert true_images.size(0) == true_box.size(0) == true_lbl.size(0)\n\n    # Store all detections in a single continuous tensor while keeping track of the image it is from\n    det_images = list()\n    for i in range(len(det_lbl)):\n        det_images.extend([i] * det_lbl[i].size(0))\n    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n    det_box = torch.cat(det_box, dimsion=0)  # (n_detections, 4)\n    det_lbl = torch.cat(det_lbl, dimsion=0)  # (n_detections)\n    det_score = torch.cat(det_score, dimsion=0)  # (n_detections)\n\n    assert det_images.size(0) == det_box.size(0) == det_lbl.size(0) == det_scores.size(0)\n\n    # Calculate APs for each class (except background)\n    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n    for c in range(1, n_classes):\n        # Extract only objects with this class\n        true_class_images = true_images[true_lbl == c]  # (n_class_objects)\n        true_class_boxes = true_boxes[true_lbl == c]  # (n_class_objects, 4)\n        true_class_difficulties = true_difficulties[true_lbl == c]  # (n_class_objects)\n        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n\n        # Keep track of which true objects with this class have already been 'detected'\n        # So far, none\n        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(device)  # (n_class_objects)\n\n        # Extract only detections with this class\n        det_class_images = det_images[det_lbl == c]  # (n_class_detections)\n        det_class_boxes = det_boxes[det_lbl == c]  # (n_class_detections, 4)\n        det_class_scores = det_scores[det_lbl == c]  # (n_class_detections)\n        n_class_detections = det_class_boxes.size(0)\n        if n_class_detections == 0:\n            continue\n\n        # Sort detections in decreasing order of confidence/scores\n        det_class_scores, sort_ind = torch.sort(det_class_scores, dimsion=0, descending=True)  # (n_class_detections)\n        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n\n        # In the order of decreasing scores, check if true or false positive\n        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n        for d in range(n_class_detections):\n            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n            this_image = det_class_images[d]  # (), scalar\n\n            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n            object_difficulties = true_class_difficulties[true_class_images == this_image]  # (n_class_objects_in_img)\n            # If no such object in this image, then the detection is a false positive\n            if object_boxes.size(0) == 0:\n                false_positives[d] = 1\n                continue\n\n            # Find maximum overlap of this detection with objects in this image of this class\n            overlaps = find_IoU(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n\n            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n            # We need 'original_ind' to update 'true_class_boxes_detected'\n\n            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n            if max_overlap.item() > 0.5:\n                # If the object it matched with is 'difficult', ignore it\n                if object_difficulties[ind] == 0:\n                    # If this object has already not been detected, it's a true positive\n                    if true_class_boxes_detected[original_ind] == 0:\n                        true_positives[d] = 1\n                        true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n                    # Otherwise, it's a false positive (since this object is already accounted for)\n                    else:\n                        false_positives[d] = 1\n            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n            else:\n                false_positives[d] = 1\n\n        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n        cumul_precision = cumul_true_positives / ( cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n\n        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n        for i, t in enumerate(recall_thresholds):\n            recalls_above_t = cumul_recall >= t\n            if recalls_above_t.any():\n                precisions[i] = cumul_precision[recalls_above_t].max()\n            else:\n                precisions[i] = 0.\n        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n\n    # Calculate Mean Average Precision (mAP)\n    mean_average_precision = average_precisions.mean().item()\n\n    # Keep class-wise average precisions in a dictionary\n    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n\n    return average_precisions, mean_average_precision","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:18.823852Z","iopub.execute_input":"2022-03-25T06:05:18.824031Z","iopub.status.idle":"2022-03-25T06:05:18.864184Z","shell.execute_reply.started":"2022-03-25T06:05:18.824004Z","shell.execute_reply":"2022-03-25T06:05:18.863539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nimport torchvision\nfrom math import sqrt\nimport torch.nn.functional as F\n\n\nclass VGGBase(nn.Module):\n    \"\"\"\n    VGG base convolutions to produce lower-level feature maps.\n    \"\"\"\n\n    def __init__(self):\n        super(VGGBase, self).__init__()\n\n        # Standard convolutional layers in VGG16\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n\n        # Replacements for FC6 and FC7 in VGG16\n        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n\n        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n\n        # Load pretrained layers\n        self.load_pretrained_layers()\n\n    def forward(self, image):\n        \"\"\"\n        Forward propagation.\n        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n        :return: lower-level feature maps conv4_3 and conv7\n        \"\"\"\n        out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)\n        out = F.relu(self.conv1_2(out))  # (N, 64, 300, 300)\n        out = self.pool1(out)  # (N, 64, 150, 150)\n\n        out = F.relu(self.conv2_1(out))  # (N, 128, 150, 150)\n        out = F.relu(self.conv2_2(out))  # (N, 128, 150, 150)\n        out = self.pool2(out)  # (N, 128, 75, 75)\n\n        out = F.relu(self.conv3_1(out))  # (N, 256, 75, 75)\n        out = F.relu(self.conv3_2(out))  # (N, 256, 75, 75)\n        out = F.relu(self.conv3_3(out))  # (N, 256, 75, 75)\n        out = self.pool3(out)  # (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\n\n        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n        conv4_3_feats = out  # (N, 512, 38, 38)\n        out = self.pool4(out)  # (N, 512, 19, 19)\n\n        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n        out = self.pool5(out)  # (N, 512, 19, 19), pool5 does not reduce dimensions\n\n        out = F.relu(self.conv6(out))  # (N, 1024, 19, 19)\n\n        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n\n        # Lower-level feature maps\n        return conv4_3_feats, conv7_feats\n\n    def load_pretrained_layers(self):\n        \"\"\"\n        As in the paper, we use a VGG-16 pretrained on the ImageNet task as the base network.\n        https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n        However, the original VGG-16 does not contain the conv6 and con7 layers.\n        Therefore, we convert fc6 and fc7 into convolutional layers, and subsample by decimation.\n        \"\"\"\n        # Current state of base\n        state_dict = self.state_dict()\n        param_names = list(state_dict.keys())\n\n        # Pretrained VGG base\n        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n        pretrained_param_names = list(pretrained_state_dict.keys())\n\n        # Transfer conv. parameters from pretrained model to current model\n        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n\n        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n        # fc6\n        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n        # fc7\n        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n\n        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n        # ...operating on the 2D image of size (C, H, W) without padding\n\n        self.load_state_dict(state_dict)\n\n        print(\"\\nLoaded base model.\\n\")\n        \n\nclass AuxiliaryConvolutions(nn.Module):\n    def __init__(self):\n        super(AuxiliaryConvolutions, self).__init__()\n        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n\n        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n\n        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n\n        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n        \n        self.init_conv2d()\n        \n    def init_conv2d(self):\n        \"\"\"\n        Initialize convolution parameters.\n        \"\"\"\n        for c in self.children():\n            if isinstance(c, nn.Conv2d):\n                nn.init.xavier_uniform_(c.weight)\n                nn.init.constant_(c.bias, 0.)\n        \n    def forward(self, conv7_feats):\n        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n        conv8_2_feats = out  # (N, 512, 10, 10)\n\n        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n        conv9_2_feats = out  # (N, 256, 5, 5)\n\n        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n        conv10_2_feats = out  # (N, 256, 3, 3)\n\n        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n\n        # Higher-level feature maps\n        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n    \nclass PredictionConvolutions(nn.Module):\n    def __init__(self, n_classes):\n        super(PredictionConvolutions, self).__init__()\n        self.n_classes = n_classes\n        \n        n_boxes = {'conv4_3': 4,\n                   'conv7': 6,\n                   'conv8_2': 6,\n                   'conv9_2': 6,\n                   'conv10_2': 4,\n                   'conv11_2': 4}\n        \n        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n\n        # Class prediction convolutions (predict classes in localization boxes)\n        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n        \n        self.init_conv2d()\n        \n    def init_conv2d(self):\n        for c in self.children():\n            if isinstance(c, nn.Conv2d):\n                nn.init.xavier_uniform_(c.weight)\n                nn.init.constant_(c.bias, 0.)\n\n    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n        batch_size = conv4_3_feats.size(0)\n\n        # Predict localization boxes' bounds (as offsets w.r.t prior-boxes)\n        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)  # (N, 16, 38, 38)\n        l_conv4_3 = l_conv4_3.permute(0, 2, 3,\n                                      1).contiguous()  # (N, 38, 38, 16), to match prior-box order (after .view())\n        # (.contiguous() ensures it is stored in a contiguous chunk of memory, needed for .view() below)\n        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)  # (N, 5776, 4), there are a total 5776 boxes on this feature map\n\n        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n\n        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n\n        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n\n        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n\n        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n\n        # Predict classes in localization boxes\n        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n                                      1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n                                   self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n\n        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n        c_conv7 = c_conv7.view(batch_size, -1,\n                               self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n\n        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n\n        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n\n        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n\n        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n\n        # A total of 8732 boxes\n        # Concatenate in this specific order (i.e. must match the order of the prior-boxes)\n        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2], dim=1)  # (N, 8732, n_classes)\n\n        return locs, classes_scores\n    \nclass SSD(nn.Module):\n    def __init__(self, n_classes=2):\n        super(SSD, self).__init__()\n        self.n_classes = n_classes\n        \n        self.base = VGGBase().cuda()\n        self.aux_convs = AuxiliaryConvolutions().cuda()\n        self.pred_convs = PredictionConvolutions(n_classes).cuda()\n        \n        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))  # there are 512 channels in conv4_3_feats\n        nn.init.constant_(self.rescale_factors, 20)\n\n        # Prior boxes\n        self.prior_cxcy = self.create_prior_boxes()\n        \n    def forward(self, image):\n        \"\"\"\n        Forward propagation.\n        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n        \"\"\"\n        # Run VGG base network convolutions (lower level feature map generators)\n        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n\n        # Rescale conv4_3 after L2 norm\n        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n        # (PyTorch autobroadcasts singleton dimensions during arithmetic)\n\n        # Run auxiliary convolutions (higher level feature map generators)\n        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n            self.aux_convs(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n\n        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n                                               conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n\n        return locs, classes_scores\n\n    \n    def create_prior_boxes(self):\n        \"\"\"\n        Creating the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n        \"\"\"\n        fmap_dims = {'conv4_3': 38,\n                     'conv7': 19,\n                     'conv8_2': 10,\n                     'conv9_2': 5,\n                     'conv10_2': 3,\n                     'conv11_2': 1}\n\n        obj_scales = {'conv4_3': 0.1,\n                      'conv7': 0.2,\n                      'conv8_2': 0.375,\n                      'conv9_2': 0.55,\n                      'conv10_2': 0.725,\n                      'conv11_2': 0.9}\n\n        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n                         'conv7': [1., 2., 3., 0.5, .333],\n                         'conv8_2': [1., 2., 3., 0.5, .333],\n                         'conv9_2': [1., 2., 3., 0.5, .333],\n                         'conv10_2': [1., 2., 0.5],\n                         'conv11_2': [1., 2., 0.5]}\n\n        fmaps = list(fmap_dims.keys())\n\n        prior_boxes = []\n\n        for k, fmap in enumerate(fmaps):\n            for i in range(fmap_dims[fmap]):\n                for j in range(fmap_dims[fmap]):\n                    cx = (j + 0.5) / fmap_dims[fmap]\n                    cy = (i + 0.5) / fmap_dims[fmap]\n\n                    for ratio in aspect_ratios[fmap]:\n                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n\n                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n                        # scale of the current feature map and the scale of the next feature map\n                        if ratio == 1.:\n                            try:\n                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n                            # For the last feature map, there is no \"next\" feature map\n                            except IndexError:\n                                additional_scale = 1.\n                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n\n        prior_boxes = torch.FloatTensor(prior_boxes).cuda()  # (8732, 4)\n        prior_boxes.clamp_(0, 1)  # (8732, 4)\n\n        return prior_boxes\n    \n    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n        \"\"\"\n        Decipher the 8732 locations and class scores (output of ths SSD300) to detect objects.\n        For each class, perform Non-Maximum Suppression (NMS) on boxes that are above a minimum threshold.\n        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n        :param min_score: minimum threshold for a box to be considered a match for a certain class\n        :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via NMS\n        :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n        :return: detections (boxes, labels, and scores), lists of length batch_size\n        \"\"\"\n        batch_size = predicted_locs.size(0)\n        n_priors = self.priors_cxcy.size(0)\n#         print(n_priors)\n        predicted_scores = F.softmax(predicted_scores, dim=2)  # (N, 8732, n_classes)\n\n        # Lists to store final predicted boxes, labels, and scores for all images\n        all_images_boxes = list()\n        all_images_labels = list()\n        all_images_scores = list()\n\n        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n\n        for i in range(batch_size):\n            # Decode object coordinates from the form we regressed predicted boxes to\n            decoded_locs = cxcy_to_xy(\n                gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  # (8732, 4), these are fractional pt. coordinates\n\n            # Lists to store boxes and scores for this image\n            image_boxes = list()\n            image_labels = list()\n            image_scores = list()\n\n            max_scores, best_label = predicted_scores[i].max(dim=1)  # (8732)\n\n            # Check for each class\n            for c in range(1, self.n_classes):\n                # Keep only predicted boxes and scores where scores for this class are above the minimum score\n                class_scores = predicted_scores[i][:, c]  # (8732)\n                score_above_min_score = class_scores > min_score  # torch.uint8 (byte) tensor, for indexing\n                n_above_min_score = score_above_min_score.sum().item()\n                if n_above_min_score == 0:\n                    continue\n                class_scores = class_scores[score_above_min_score]  # (n_qualified), n_min_score <= 8732\n                class_decoded_locs = decoded_locs[score_above_min_score]  # (n_qualified, 4)\n\n                # Sort predicted boxes and scores by scores\n                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n\n                # Find the overlap between predicted boxes\n                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  # (n_qualified, n_min_score)\n\n                # Non-Maximum Suppression (NMS)\n\n                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n                # 1 implies suppress, 0 implies don't suppress\n                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).cuda()  # (n_qualified)\n\n                # Consider each box in order of decreasing scores\n                for box in range(class_decoded_locs.size(0)):\n                    # If this box is already marked for suppression\n                    if suppress[box] == 1:\n                        continue\n\n                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n                    # Find such boxes and update suppress indices\n                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n\n                    # Don't suppress this box, even though it has an overlap of 1 with itself\n                    suppress[box] = 0\n\n                # Store only unsuppressed boxes for this class\n                image_box.append(class_decoded_locs[1 - suppress])\n                image_lbl.append(torch.LongTensor((1 - suppress).sum().item() * [c]).cuda())\n                image_score.append(class_scores[1 - suppress])\n\n            # If no object in any class is found, store a placeholder for 'background'\n            if len(image_box) == 0:\n                image_box.append(torch.FloatTensor([[0., 0., 1., 1.]]).cuda())\n                image_lbl.append(torch.LongTensor([0]).cuda())\n                image_score.append(torch.FloatTensor([0.]).cuda())\n\n            # Concatenate into single tensors\n            image_box = torch.cat(image_box, dim=0)  # (n_objects, 4)\n            image_lbl = torch.cat(image_lbl, dim=0)  # (n_objects)\n            image_score = torch.cat(image_score, dim=0)  # (n_objects)\n            n_objects = image_score.size(0)\n\n            # Keep only the top k objects\n            if n_objects > top_k:\n                image_score, sort_ind = image_scores.sort(dim=0, descending=True)\n                image_score = image_scores[:top_k]  # (top_k)\n                image_box = image_box[sort_ind][:top_k]  # (top_k, 4)\n                image_lbl = image_lbl[sort_ind][:top_k]  # (top_k)\n\n            # Append to lists that store predicted boxes and scores for all images\n            all_images_box.append(image_box)\n            all_images_lbl.append(image_lbl)\n            all_images_score.append(image_score)\n\n        return all_images_box, all_images_lbl, all_images_score  # lists of length batch_size","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:18.865722Z","iopub.execute_input":"2022-03-25T06:05:18.866099Z","iopub.status.idle":"2022-03-25T06:05:18.953737Z","shell.execute_reply.started":"2022-03-25T06:05:18.866062Z","shell.execute_reply":"2022-03-25T06:05:18.953015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiBoxLoss(nn.Module):\n    \"\"\"\n    The MultiBox loss, a loss function for object detection.\n    This is a combination of:\n    (1) a localization loss for the predicted locations of the boxes, and\n    (2) a confidence loss for the predicted class scores.\n    \"\"\"\n\n    def __init__(self, prior_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n        super(MultiBoxLoss, self).__init__()\n        self.prior_cxcy = prior_cxcy\n        self.prior_xy = ctrx_ctry_to_xy(prior_cxcy)\n        self.threshold = threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.alpha = alpha\n\n        self.smooth_l1 = nn.L1Loss()\n        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n        \n    def forward(self, predicted_locs, predicted_scores, box, lbl):\n        \"\"\"\n        Forward propagation.\n       \n        \"\"\"\n        batch_size = predicted_locs.size(0)\n        n_priors = self.prior_cxcy.size(0)\n        n_classes = predicted_scores.size(2)\n\n        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n\n        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).cuda()  # (N, 8732, 4)\n        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).cuda()  # (N, 8732)\n\n        # For each image\n        for i in range(batch_size):\n            n_objects = box[i].size(0)\n\n            overlap = find_IoU(box[i], self.prior_xy)  # (n_objects, 8732)\n\n            # For each prior, find the object that has the maximum overlap\n            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n\n            # We don't want a situation where an object is not represented in our positive (non-background) priors -\n            # 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.\n            # 2. All priors with the object may be assigned as background based on the threshold (0.5).\n\n            # To remedy this -\n            # First, find the prior that has the maximum overlap for each object.\n            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n\n            # Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).cuda()\n\n            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n            overlap_for_each_prior[prior_for_each_object] = 1.\n\n            # Labels for each prior\n            label_for_each_prior = lbl[i][object_for_each_prior]  # (8732)\n            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n\n            # Store\n            true_classes[i] = label_for_each_prior\n\n            # Encode center-size object coordinates into the form we regressed predicted boxes to\n            true_locs[i] = ctrx_ctry_to_gcxgcy(xy_to_ctrx_ctry(box[i][object_for_each_prior]), self.prior_cxcy)  # (8732, 4)\n\n        # Identify priors that are positive (object/non-background)\n        positive_priors = true_classes != 0  # (N, 8732)\n\n        # LOCALIZATION LOSS\n\n        # Localization loss is computed only over positive (non-background) priors\n        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n\n        #  if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n\n        # CONFIDENCE LOSS\n\n        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n        # we will take the hardest (neg_pos_ratio * n_positives) negative priors for each image, i.e where there is maximum loss\n        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n\n        # Number of positive and hard-negative priors per image\n        n_positives = positive_priors.sum(dim=1)  # (N)\n        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n\n        # First, find the loss for all priors\n        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n\n        # We already know which priors are positive\n        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n\n        # Next, find which priors are hard-negative\n        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).cuda()  # (N, 8732)\n        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n\n        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n\n        # TOTAL LOSS\n\n        return conf_loss + self.alpha * loc_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:18.955668Z","iopub.execute_input":"2022-03-25T06:05:18.955936Z","iopub.status.idle":"2022-03-25T06:05:18.974081Z","shell.execute_reply.started":"2022-03-25T06:05:18.955902Z","shell.execute_reply":"2022-03-25T06:05:18.973218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SSD().cuda()\ncriterion = MultiBoxLoss(prior_cxcy=model.prior_cxcy).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:19.135024Z","iopub.execute_input":"2022-03-25T06:05:19.135212Z","iopub.status.idle":"2022-03-25T06:05:26.641657Z","shell.execute_reply.started":"2022-03-25T06:05:19.135189Z","shell.execute_reply":"2022-03-25T06:05:26.640923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\n\nImage_path = '/kaggle/input/colorful-fashion-dataset-for-object-detection/colorful_fashion_dataset_for_object_detection/JPEGImages/'\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\ndef create():\n    name = []\n    for dirname, _,filenames in os.walk(Image_path):\n        for filename in filenames:\n            name.append(filename.split('.')[0])\n            \n    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n\ndf = create()\nprint('Total image: ', len(df))\n\nx_trainval, x_test = train_test_split(df['id'].values, test_size=0.1, random_state = 19)\nx_train, x_val = train_test_split(x_trainval, test_size=0.15, random_state=19)\nprint('Train Size :', len(x_train))\nprint('Val Size :', len(x_val))\nprint('Test Size :', len(x_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:26.643121Z","iopub.execute_input":"2022-03-25T06:05:26.643803Z","iopub.status.idle":"2022-03-25T06:05:28.719254Z","shell.execute_reply.started":"2022-03-25T06:05:26.643748Z","shell.execute_reply":"2022-03-25T06:05:28.718468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_data = Dateset(x_train, transform=transform)\ntrain_load = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=train_data.collate_fn)\n\nvalid_data = Dateset(x_val, transform=transform)\nvalid_load = DataLoader(valid_data, batch_size=2, shuffle=True, collate_fn=valid_data.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:28.720613Z","iopub.execute_input":"2022-03-25T06:05:28.720862Z","iopub.status.idle":"2022-03-25T06:05:28.725519Z","shell.execute_reply.started":"2022-03-25T06:05:28.720831Z","shell.execute_reply":"2022-03-25T06:05:28.724842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport time\n\nfor epoch in range(1, 3):\n    model.train()\n    train_loss = []\n    for step, (img, box, lbl) in enumerate(train_load):\n        time_1 = time.time()\n        img = img.cuda()\n#         box = torch.cat(box)\n        box = [b.cuda() for b in box]\n#         label = torch.cat(label)\n        lbl = [l.cuda() for l in lbl]\n        \n        pred_loc, pred_sco = model(img)\n        \n        loss = criterion(pred_loc, pred_sco, box, lbl)\n        \n         # Backward prop.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n        print_feq = 100\n        if step % print_feq == 0:\n            print('epoch:', epoch, \n                  '\\tstep:', step+1, '/', len(train_load) + 1,\n                  '\\ttrain loss:', '{:.4f}'.format(loss.item()),\n                  '\\ttime:', '{:.4f}'.format((time.time()-time_1)*print_feq), 's')\n    \n    model.eval();\n    valid_loss = []\n    for step, (img, box, lbl) in enumerate(tqdm(valid_load)):\n        img = img.cuda()\n        box = [b.cuda() for b in box]\n        lbl = [l.cuda() for l in lbl]\n        pred_loc, pred_sco = model(img)\n        loss = criterion(pred_loc, pred_sco, box, lbl)\n        valid_loss.append(loss.item())\n        \n    print('epoch:', epoch, '/', 3,\n            '\\ttrain loss:', '{:.4f}'.format(np.mean(train_loss)),\n            '\\tvalid loss:', '{:.4f}'.format(np.mean(valid_loss)))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:05:28.727437Z","iopub.execute_input":"2022-03-25T06:05:28.727876Z","iopub.status.idle":"2022-03-25T06:08:30.493199Z","shell.execute_reply.started":"2022-03-25T06:05:28.727837Z","shell.execute_reply":"2022-03-25T06:08:30.492488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = []\nfor dirname, _,filenames in os.walk(Image_path):\n    for filename in filenames:\n        name.append(filename.split('.')[0])\n            \npd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:08:30.494476Z","iopub.execute_input":"2022-03-25T06:08:30.494894Z","iopub.status.idle":"2022-03-25T06:08:30.874908Z","shell.execute_reply.started":"2022-03-25T06:08:30.494856Z","shell.execute_reply":"2022-03-25T06:08:30.874287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef test(n=5):\n    d = []\n    for i in range(5):\n        img = Image.open('/kaggle/input/colorful-fashion-dataset-for-object-detection/colorful_fashion_dataset_for_object_detection/JPEGImages/100254.jpg')\n        img = transform(image)\n\n        img = img.cuda()\n        predicted_locs, predicted_scores = model(img.unsqueeze(0))\n        det_box, det_lbl, det_scores = model.detect_objects(predicted_locs, predicted_scores, min_score=0.2,\n                                                                 max_overlap=0.5, top_k=200)\n        det_box = det_box[0].to('cuda')\n\n        dimsion = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n        det_box = det_box * dimsion\n\n        annotated_image = image\n        draw = ImageDraw.Draw(annotated_image)\n\n        box_location = det_box[0].tolist()\n        draw.rectangle(xy=box_location, outline='red')\n        draw.rectangle(xy=list(map(lambda x:x+1, box_location)), outline='red')\n        d.append(annotated_image)\n    return d","metadata":{"execution":{"iopub.status.busy":"2022-03-25T06:08:30.876709Z","iopub.execute_input":"2022-03-25T06:08:30.877089Z","iopub.status.idle":"2022-03-25T06:08:30.885239Z","shell.execute_reply.started":"2022-03-25T06:08:30.877055Z","shell.execute_reply":"2022-03-25T06:08:30.88444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}